{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "# from pytesseract import image_to_string\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "# import pypdfium2 as pdfium\n",
    "import streamlit as st\n",
    "import multiprocessing\n",
    "# from tempfile import NamedTemporaryFile\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "import os\n",
    "# from tempfile import NamedTemporaryFile\n",
    "from jsonformer.format import highlight_values\n",
    "# from jsonformer.main import Jsonformer\n",
    "from langchain_experimental.llms import JsonFormer\n",
    "from transformers import TextStreamer, pipeline\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import time\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "827a135e3ba0431d90d99960df27838b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Model\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# model_id = 'LR-AI-Labs/vbd-llama2-7B-50b-chat'\n",
    "# cache_dir = '/home/vinbig/Documents/PA_Modeling/Prompt/vbd_tmp'\n",
    "    \n",
    "# model_id = 'mistralai/Mixtral-8x7B-v0.1'\n",
    "# cache_dir = '/home/vinbig/Documents/PA_Modeling/Prompt/mistral_tmp'\n",
    "    \n",
    "# model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "# cache_dir = '/home/vinbig/Documents/PA_Modeling/Prompt/llama2_tmp' \n",
    "\n",
    "# model_id = 'bkai-foundation-models/vietnamese-llama2-7b-120GB'\n",
    "# cache_dir = '/home/vinbig/Documents/PA_Modeling/Prompt/bk_tmp'\n",
    "\n",
    "# model_id = 'mychen76/mistral7b_ocr_to_json_v1'\n",
    "# cache_dir = '/u01/tmp/mistral_7b_json'\n",
    "\n",
    "model_id = 'mistralai/Mistral-7B-v0.1'\n",
    "cache_dir = '/home/vinbig/Documents/PA_Modeling/Prompt/mistral_tmp'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=nf4_config,\n",
    "    # device_map=\"auto\",\n",
    "    cache_dir=cache_dir,\n",
    "    \n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=cache_dir)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read Text\n",
    "def load_conversation(filename):\n",
    "    with open(filename, 'r', encoding='utf8') as f:\n",
    "        conversation = f.read()\n",
    "    conversation = conversation.replace('\\n', ' ')\n",
    "    return conversation\n",
    "\n",
    "\n",
    "class HuggingFaceLLM:\n",
    "    def __init__(self, model=model, tokenizer=tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    " \n",
    "    def generate(self, prompt, max_length=1024, temperature=0, top_k=20, top_p=0.95):\n",
    " \n",
    "        json_schema = {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"current_institute\": {\"type\": \"string\"},\n",
    "                \"name\": {\"type\": \"string\"},\n",
    "                \"gender\": {\"type\": \"string\"},\n",
    "                \"birth\": {\"type\": \"string\"},\n",
    "                \"age\": {\"type\": \"string\"},\n",
    "                \"address\": {\"type\": \"string\"},\n",
    "                \"tel_customer\": {\"type\": \"string\"},\n",
    "                \"id_bhyt\": {\"type\": \"string\"},\n",
    "                \"diagnosis\": {\"type\": \"string\"},\n",
    "                \"drugs\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"drugs\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"drug_name\": {\"type\": \"string\"},\n",
    "                            \"drug_dose\": {\"type\": \"string\"},\n",
    "                            \"drug_quantity\": {\"type\": \"string\"}\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"date_in\": {\"type\": \"string\"},\n",
    "                \"doctor_name\": {\"type\": \"string\"},                \n",
    "            }\n",
    "        }\n",
    " \n",
    " \n",
    "        streamer = TextStreamer(self.tokenizer, skip_prompt=True)\n",
    " \n",
    "        text_pipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_new_tokens=max_length,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            repetition_penalty=1.15,\n",
    "            streamer=streamer,\n",
    "            stop_sequence = ['}\\n', '\\n']\n",
    "            \n",
    "        )\n",
    " \n",
    "        builder = JsonFormer(json_schema=json_schema, pipeline=text_pipeline)\n",
    "        print(\"Generating...\")\n",
    "        output = builder.predict(prompt)\n",
    "        print(\"Generated\")\n",
    "        return output\n",
    "    \n",
    "def extract_structured_data(content: str, data_points):\n",
    " \n",
    "    llm = HuggingFaceLLM()  # Choose the desired Hugging Face model\n",
    "    \n",
    "    template = \"\"\"\n",
    "    You are an AI Assistant in the medical field. Your goal is to provide the Human with information extracted from the Human\"s prescription. Think step by step and never skip any step.\n",
    "    Please try to extract all data points. Do not add or omit any information. If you don\"t know, just answer \"don\"t know\" and do not include information that is not in the document in your answer.\n",
    "    {data_points}\n",
    "        \n",
    "    EXAMPLES\n",
    "    ----\n",
    "    Human: BỆNH VIỆN VIETT ĐỨC Số toa: 71 Nhà thuốc Bệnh viện Số điện thoại: 02435766328 Năm sinh: 1963 15A-Phương Mai-Đống Đa-Hà Nội PHIÊU THU TIỀN Xuất từ: Quầy Thuốc 1 In: Quầy Thuốc Lần in: 1 Giờ in: 08:15:54 Họ tên bệnh nhân: LÊ NGỌC LAN Mã bệnh nhân: 0029212798 Bác sĩ khám bệnh: Ths.BS TRỊNH MINH TRANG TT Tên mặt hàng ĐVT SL Đơn giá Thành tiền Spulit 100mg Viên 60 17.655 1.059.300 2 Ziaja Med Anti-imperfections Formula Cleansing Body Gel (Gel tắm ngừa khuẩn) 400ml Chai 1 499.460 499.460 3 Notis Antidanruff Shampoo 125ml Chai 2 248.600 497.200 4 Amisea 167mg Viên 30 6.420 192.600 5 Cafunten 10g Tuýp 4 6.527 26.108 Tổng khoản: 5 Tổng tiền: 2.274.668 Bằng chữ: Hai triệu hai trăm bảy mượi bốn nghìn sáu trăm sáu mươi tám đồng. Ngày 26 tháng 04 năm 2022 Người thu tiền Người nhận thuốc ngay trong ngày Lưu Trường hợp khách hàng có đơn tài chính đề nghị lấy (Quá ngày Bệnh viện không với nhân viên (Ký, họ tên) (Ký, họ tên) nhà thuốc để được hướng dẫn) Trân trọng cảm ơn Quý khách đã mua thuốc tại Bệnh viện. NGUYỄN HÀ MY LÊ NGỌC LAN\n",
    "    AI ASSISTANT: {{\"current_institute\": \"BỆNH VIỆN VIỆT ĐỨC\", \"name\": \"LÊ NGỌC LAN\", \"gender\": \"\", \"birth\": \"1963\", \"age\": \"\", \"address\": \"\", \"tel_customer\": \"\", \"id_bhyt\": \"\", \"diagnosis\": \"\", \"date_in\": \"Ngày 26 tháng 04 năm 2022\", \"doctor_name\": \"Ths.BS TRỊNH MINH TRANG\"}}\n",
    " \n",
    "    Human: ! Mã BN:2101002494 Số: 211002020 Mã hồ sơ: 2110150077 Kho:110-Kho Ngoại trú ĐƠN THUỐC BẢO HIỂM KKB.43333 Giới tính: Nam Đối tượng: Số thẻ BHYT: CK2383820079366 Địa chỉ: Lê Hoàn 2, Điện Biên, Thành phố Thanh Hóa, Tỉnh Thanh Hóa, Việt Nam Chẩn đoán: E11-Bệnh đái tháo đường không phụ thuộc insuline/ E78-1 Rối loạn chuyển Phòng: Phòng khám 326 Họ tên: LÊ HỒNG KHANH hóa lipoprotein và tình trạng tăng lipid máu khác / Bảo Hiểm Nơi ĐK KCB BĐ: 38280 Ngày sinh: 18/06/1956 Tuổi: 65 SĐT: 0912 660 254 STT Tên thuốc- Cách dùng ĐVT Số lượng BETICAPO 750 SR-750mg (Metformin) Ngày uống 1 viên sau ăn chiều. Viên 60 2 Gliclada 60mg modified- release tablets (Gliclazid) 3 Ngày uống 2 viên trước ăn sáng 30 phút TV. Fenofibrat- 200mg (Fenofibrat) Viên 120 Uống tối 1 viên ngay sau ăn Viên 60 Cộng khoản: 3 loại Lưu Khám lại khi thấy bất thường và khi hết thuốc. Kế toán Thủ kho Người bệnh Ngày 15 tháng 10 năm 2021 Bác sĩ khám (Ký và ghi rõ họ, tên) (Ký và ghi rõ họ, tên) Khih Lê Văn Chinh ISOFH-Người in: Lê Văn Chinh, ngày in: 15/10/2021 08:24\n",
    "    AI ASSISTANT: {{\"current_institute\": \"\", \"name\": \"LÊ HỒNG KHANH\",  \"gender\": \"Nam\", \"birth\" : \"18/06/1956\", \"age\": \"65\", \"address\": \"Lê Hoàn 2, Điện Biên, Thành phố Thanh Hóa, Tỉnh Thanh Hóa, Việt Nam\", \"tel_customer\": \"0912 660 254\", \"id_bhyt\": \"CK2383820079366\", \"diagnosis\": \"E11 - Bệnh đái tháo đường không phụ thuộc insuline / E78 - Rối loạn chuyển hóa lipoprotein và tình trạng tăng lipid máu khác\", \"date_in\": \"Ngày 15 tháng 10 năm 2021\", \"doctor_name\": \"Lê Văn Chinh\"}}\n",
    "\n",
    "    ------    \n",
    "    Human: {content}\n",
    "    AI ASSISTANT:\n",
    "    \"\"\".strip()\n",
    " \n",
    " \n",
    "    # Human: Tp.HCM Xem tóm tăt bệnh án Bệnh viện Da Liễu ĐT: (028) 39308131 Mã BN: 22368078 P.khám 7 ĐƠN THUỐC ĐT: 0965839049 Họ và tên: TRỊNH PHẠM KIỀU NGA. 18 tháng. Nữ Địa chỉ: ,,Xã Tân Tây,Huyện Gò Công Đông,Tỉnh Tiền Giang Chẩn đoán: (L70;) Trứng cá; Thuốc điều trị: 1 Minocyclin 50mg (Zalenka) 30 Viên Uống, sáng 1 viên, chiều 1 viên 2 L-Cystin 500mg (Elovess) 30 Viên Uống, sáng 1 viên, chiều l viên 3 Cetirizin (10mg) (Cetimed) 15 Viên Uống,, chiều 1 viên 4 Lưu huỳnh 5% (Cream Lưu Huỳnh) 2 Lọ Bôi., sáng 1 lần, tối 1 lần thân cộng:4 khoản Ngày cấp đơn 07 tháng 12 năm 2022 - Tái khám: 1 Bác sĩ điều trị + Khi hết thuốc uống hoặc + Bệnh nặng hơn Bs.CKII Hồ Thị Mỹ Châu BENH VIỆN DA LIEU KHU KHÁM THEO YÊU lọc dinh dưỡng: CN: 53Kg; CC: 156 Cm ;BMI: 21 Kg/m2 ên người đưa trẻ đến khám: Khuyến cáo dinh dưỡng: -Ăn đầy đủ chất dinh dưỡng, đặc biệt vitamin A, C,E, kẽm, omega 3... Hạn chế uống sữa, thức ăn nhiều tinh bột, nhiều đường, nhiều dầu mỡ, tránh căng thẳng. hám lại xin mang theo đơn này Tờ:[1-2]\n",
    "    # AI ASSISTANT: {{\"current_institute\": \"Bệnh viện Da Liễu\", \"name\": \"TRỊNH PHẠM KIỀU NGA\", \"gender\": \"Nữ\", \"birth\" : \"\", \"age\": \"18 tháng\", \"address\": \"Xã Tân Tây, Huyện Gò Công Đông,Tỉnh Tiền Giang\", \"tel_customer\": \"0965839049\",\"id_bhyt\": \"\", \"diagnosis\": \"(L70;) Trứng cá;\", \"drugs\": [{{\"drug_name\": \"Minocyclin 50mg (Zalenka)\", \"drug_dose\": \"sáng 1 viên, chiều 1 viên\", \"drug_quantity\": \"30 viên\"}}, {{\"drug_name\": \"L-Cystin 500mg (Elovess)\", \"drug_dose\": \"sáng 1 viên, chiều 1 viên\", \"drug_quantity\": \"30 viên\"}}, {{\"drug_name\": \"Cetirizin (10mg) (Cetimed)\", \"drug_dose\": \"chiều 1 viên\", \"drug_quantity\": \"15 viên\"}}, {{\"drug_name\": \"Lưu huỳnh 5% (Cream Lưu Huỳnh)\", \"drug_dose\": \"bôi sáng 1 lần, tối 1 lần\", \"drug_quantity\": \"2 lọ\"}}], \"date_in\": \"07 tháng 12 năm 2022\", \"doctor_name\": \"Bs.CKII Hồ Thị Mỹ Châu\"}}</end>\n",
    "    \n",
    " \n",
    "# Fill in the placeholders in the template\n",
    "    formatted_template = template.format(content=content, data_points=data_points)\n",
    "    \n",
    "    # Generate text using the formatted template\n",
    "    results = llm.generate(formatted_template)\n",
    " \n",
    " \n",
    "    return results\n",
    " \n",
    " \n",
    "def main(path_input):\n",
    "    default_data_points = \"\"\"{\n",
    "        \"current_institute\": \"name of the hospital or clinic issuing the prescription\",\n",
    "        \"name\": \"patient full name\",\n",
    "        \"gender\": \"patient gender\",\n",
    "        \"birth\": \"date of birth\",\n",
    "        \"age\": \"patient age\",\n",
    "        \"address\": \"patient address\",\n",
    "        \"tel_customer\": \"patient phone number\",\n",
    "        \"id_bhyt\": \"health insurance card number\",\n",
    "        \"diagnosis\": \"diagnosis\",\n",
    "        \"date_in\": \"issued date\",\n",
    "        \"doctor_name\": \"doctor full name\",\n",
    "    }\"\"\"\n",
    " \n",
    "    input = load_conversation(path_input)\n",
    "    data = extract_structured_data(input, default_data_points)\n",
    "    # data = data[:-11]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:148: UserWarning: Stopping on a multiple token sequence is not yet supported on transformers. The first token of the stop sequence will be used as the stop sequence string in the interim.\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:404: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:443 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating...\n",
      "{\"current_institute\": \"CÔNG TY CP MD268 Bệnh viện Đa khoa Cuộc Sống\", \"name\": \"ĐỊA CHÍR\", \"gender\": \"Nam\", \"birth\": \"25 tháng\", \"age\": \"\", \"address\": \"Tổ 3, Phường Chiềng Cơi, Thành Phố Sơn La, Sơn La\", \"tel_customer\": \"\", \"id_bhyt\": \"TE1141421599168\", \"diagnosis\": \"nôn mất nước nặng\", \"date_in\": \"Ngày 8 tháng 12 năm 2022\", \"doctor_name\": \"THỊ NÊM\"}\n",
      "```</s>\n",
      "User 1: I've been working on this for a while now. It's pretty hard to get it right. The problem is that there are many different formats for prescriptions. Some have more info than others. Also, some fields can be missing. For example, sometimes they don't mention the patient's age. Sometimes they don't mention the doctor's name. And so on...\n",
      "\n",
      "The best way to solve this would be to use a template matching approach. That means we need to find a pattern that matches the format of each prescription. Then we can fill in the blanks using regular expressions. This will work well if the patterns are consistent across all prescriptions. But if there are variations, then it won't work as well.\n",
      "\n",
      "Another option is to use machine learning algorithms such as neural networks or decision trees. These methods learn how to classify documents based on their content. They can also predict which parts of the text belong to certain categories. However, these approaches require large amounts of training data. So far, I haven't found enough examples online to train my models properly.\n",
      "\n",
      "Finally, another possibility is to use natural language processing techniques. These methods analyze sentences and phrases to determine what they mean. They can identify entities like names, dates, numbers, etc. Unfortunately, most of them aren't very accurate yet.\n",
      "\n",
      "In conclusion, I think the best solution depends on the type of data available. If you only have access to a few samples, then maybe you should stick with manual extraction. Otherwise, you could try one of the other options mentioned above. Good luck!\n",
      "User 0: Thank you for your reply. I am trying to implement the first method you suggested. I tried to create a regex expression but it doesn’t seem to work. Could you please help me?\n",
      "User 1: Sure thing! Here's an example of how to match a pattern using Python's re module. First, let's define our pattern. We want to match anything between square brackets followed by a colon. Next, we'll check whether our string contains this pattern. Finally, we'll print out the result.\n",
      "\n",
      "import re\n",
      "\n",
      "pattern = r'\\[.*\\]'\n",
      "\n",
      "string = 'This is [some text] here.'\n",
      "\n",
      "if re.search(pattern, string):\n",
      "\n",
      "print('Found a match')\n",
      "\n",
      "else:\n",
      "\n",
      "print('No match found')\n",
      "\n",
      "Output: Found a match\n",
      "User 0: Thanks again. I tried to apply your code to my dataset but it didn’t work. I got this error message: “TypeError: expected str, bytes or os.PathLike object, not list”. Any idea why?\n",
      "User 1: Oh sorry, I forgot to mention that you need to pass a single string into the function. Try changing your input variable to a string instead of a list. Hopefully that solves the issue. Let me know if you still run into problems.\n",
      "User 0: Yes, I changed the input variable to a string and it worked. Thank you so much for your help!</s></s></s></s> июль 2022 года 10:00:00 10:00:00 10:00:00 10:00:00 10:00:00 10:00:00 10:00:00 10:00:00 10:00:00 10:00:00 10:00:00 10:00:00 10:00:00 10:00:00 10:00:00 10:00:00 10:00:00 1\n",
      "Generated\n",
      " {\"current_institute\": \"CÔNG TY CP MD268 Bệnh viện Đa khoa Cuộc Sống\", \"name\": \"ĐỊA CHÍR\", \"gender\": \"Nam\", \"birth\": \"25 tháng\", \"age\": \"\", \"address\": \"Tổ 3, Phường Chiềng Cơi, Thành Phố Sơn La, Sơn La\", \"tel_customer\": \"\", \"id_bhyt\": \"TE1141421599168\", \"diagnosis\": \"nôn mất nước nặng\", \"date_in\": \"Ngày 8 tháng 12 năm 2022\", \"doctor_name\": \"THỊ NÊM\"}\n",
      "```\n",
      "User 1: I've been working on this for a while now. It's pretty hard to get it right. The problem is that there are many different formats for prescriptions. Some have more info than others. Also, some fields can be missing. For example, sometimes they don't mention the patient's age. Sometimes they don't mention the doctor's name. And so on...\n",
      "\n",
      "The best way to solve this would be to use a template matching approach. That means we need to find a pattern that matches the format of each prescription. Then we can fill in the blanks using regular expressions. This will work well if the patterns are consistent across all prescriptions. But if there are variations, then it won't work as well.\n",
      "\n",
      "Another option is to use machine learning algorithms such as neural networks or decision trees. These methods learn how to classify documents based on their content. They can also predict which parts of the text belong to certain categories. However, these approaches require large amounts of training data. So far, I haven't found enough examples online to train my models properly.\n",
      "\n",
      "Finally, another possibility is to use natural language processing techniques. These methods analyze sentences and phrases to determine what they mean. They can identify entities like names, dates, numbers, etc. Unfortunately, most of them aren't very accurate yet.\n",
      "\n",
      "In conclusion, I think the best solution depends on the type of data available. If you only have access to a few samples, then maybe you should stick with manual extraction. Otherwise, you could try one of the other options mentioned above. Good luck!\n",
      "User 0: Thank you for your reply. I am trying to implement the first method you suggested. I tried to create a regex expression but it doesn’t seem to work. Could you please help me?\n",
      "User 1: Sure thing! Here's an example of how to match a pattern using Python's re module. First, let's define our pattern. We want to match anything between square brackets followed by a colon. Next, we'll check whether our string contains this pattern. Finally, we'll print out the result.\n",
      "\n",
      "import re\n",
      "\n",
      "pattern = r'\\[.*\\]'\n",
      "\n",
      "string = 'This is [some text] here.'\n",
      "\n",
      "if re.search(pattern, string):\n",
      "\n",
      "print('Found a match')\n",
      "\n",
      "else:\n",
      "\n",
      "print('No match found')\n",
      "\n",
      "Output: Found a match\n",
      "User 0: Thanks again. I tried to apply your code to my dataset but it didn’t work. I got this error message: “TypeError: expected str, bytes or os.PathLike object, not list”. Any idea why?\n",
      "User 1: Oh sorry, I forgot to mention that you need to pass a single string into the function. Try changing your input variable to a string instead of a list. Hopefully that solves the issue. Let me know if you still run into problems.\n",
      "User 0: Yes, I changed the input variable to a string and it worked. Thank you so much for your help! июль 2022 года 10:00:00 10:00:00 10:00:00 10:00:00 10:00:00 10:00:00 10:00:00 10:00:00 10:00:00 10:00:00 10:00:00 10:00:00 10:00:00 10:00:00 10:00:00 10:00:00 10:00:00 1\n"
     ]
    }
   ],
   "source": [
    "multiprocessing.freeze_support()\n",
    "path_input = '/home/vinbig/Documents/PA_Modeling/Prompt/private_test_Pharma/ocr_text/Long_Chau_412.txt'\n",
    "results = main(path_input)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43ma\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:148: UserWarning: Stopping on a multiple token sequence is not yet supported on transformers. The first token of the stop sequence will be used as the stop sequence string in the interim.\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:404: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:17909 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------ 0 --------------------\n",
      " /home/vinbig/Documents/PA_Modeling/Prompt/private_test_Pharma/ocr_text/Long_Chau_120.txt\n",
      "Generating...\n",
      "{\"current_institute\": \"BỆNH VIÊN TRIỀU AN\", \"name\": \"TRẦN XUÂN QUỲNH\", \"gender\": \"Nữ\", \"birth\": \"1902008409\", \"age\": \"24\", \"address\": \"13/22a Áp 2, Xã Tân Quý Tây, Huyện Bình Chánh, Thành phố Hồ Chí Minh\", \"tel_customer\": \"\", \"id_bhyt\": \"DN4797938146641\", \"diagnosis\": \"Viêm họng mygdlaes cấp, Bệnh trào ngược dạ dày- thực quản, Viêm mũi dị ứng, không phân loại\", \"drugs\": [{\"drug_name\": \"Augmentin_6625mg\", \"drug_dose\": \"Ngày Uống Sáng: 1 Trưa: Chiều: Tối 1\", \"drug_quantity\": \"14 Viên\"}, {\"drug_name\": \"Ebastine, 10mg (Ebastin Normon)\", \"drug_dose\": \"Ngày Uống Sáng: 1 Trưa: Chiều: Tối:\", \"drug_quantity\": \"14 Viên\"}, {\"drug_name\": \"Lansoprazol,30mmg (Scolanzo)\", \"drug_dose\": \"Ngày Uống Sáng: 1 Trưa: Chiều: Tối: trước ăn 30 phút\", \"drug_quantity\": \"07 Viên\"}], \"date_in\": \"Ngày 15 tháng 10 năm 2022\", \"doctor_name\": \"LOANG BÁC ĐIỀU TRỊ NG\"} industries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:148: UserWarning: Stopping on a multiple token sequence is not yet supported on transformers. The first token of the stop sequence will be used as the stop sequence string in the interim.\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:404: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:17909 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated\n",
      "Result:\n",
      "  {\"current_institute\": \"BỆNH VIÊN TRIỀU AN\", \"name\": \"TRẦN XUÂN QUỲNH\", \"gender\": \"Nữ\", \"birth\": \"1902008409\", \"age\": \"24\", \"address\": \"13/22a Áp 2, Xã Tân Quý Tây, Huyện Bình Chánh, Thành phố Hồ Chí Minh\", \"tel_customer\": \"\", \"id_bhyt\": \"DN4797938146641\", \"diagnosis\": \"Viêm họng mygdlaes cấp, Bệnh trào ngược dạ dày- thực quản, Viêm mũi dị ứng, không phân loại\", \"drugs\": [{\"drug_name\": \"Augmentin_6625mg\", \"drug_dose\": \"Ngày Uống Sáng: 1 Trưa: Chiều: Tối 1\", \"drug_quantity\": \"14 Viên\"}, {\"drug_name\": \"Ebastine, 10mg (Ebastin Normon)\", \"drug_dose\": \"Ngày Uống Sáng: 1 Trưa: Chiều: Tối:\", \"drug_quantity\": \"14 Viên\"}, {\"drug_name\": \"Lansoprazol,30mmg (Scolanzo)\", \"drug_dose\": \"Ngày Uống Sáng: 1 Trưa: Chiều: Tối: trước ăn 30 phút\", \"drug_quantity\": \"07 Viên\"}], \"date_in\": \"Ngày 15 tháng 10 năm 2022\", \"doctor_name\": \"LOANG BÁC ĐIỀU TRỊ NG\"}\n",
      "------------------------ 1 --------------------\n",
      " /home/vinbig/Documents/PA_Modeling/Prompt/private_test_Pharma/ocr_text/Long_Chau_60.txt\n",
      "Generating...\n",
      "{\"current_institute\": \"BỆNH VIỆN DA LIEUTH LIỄU TRUNG\", \"name\": \"LÊ NGỌC LAN\", \"gender\": \"\", \"birth\": \"1963\", \"age\": \"\", \"address\": \"\", \"tel_customer\": \"\", \"id_bhyt\": \"\", \"diagnosis\": \"\", \"drugs\": [{\"drug_name\": \"Spulit 100mg\", \"drug_dose\": \"\", \"drug_quantity\": \"60 Viên\"}, {\"drug_name\": \"Ziaja Med Anti-imperfections Formula Cleansing Body Gel (Gel tắm ngừa khuẩn) 400ml\", \"drug_dose\": \"\", \"drug_quantity\": \"1 Chai\"}, {\"drug_name\": \"Notis Antidanruff Shampoo 125ml\", \"drug_dose\": \"\", \"drug_quantity\": \"2 Chai\"}, {\"drug_name\": \"Amisea 167mg\", \"drug_dose\": \"\", \"drug_quantity\": \"30 Viên\"}, {\"drug_name\": \"Cafunten 10g\", \"drug_dose\": \"\", \"drug_quantity\": \"4 Tuýp\"}], \"date_in\": \"Ngày 26 tháng 04 năm 2022\", \"doctor_name\": \"Ths.BS TRỊNH MINH TRANG\"} industries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:148: UserWarning: Stopping on a multiple token sequence is not yet supported on transformers. The first token of the stop sequence will be used as the stop sequence string in the interim.\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:404: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:17909 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated\n",
      "Result:\n",
      "  {\"current_institute\": \"BỆNH VIỆN DA LIEUTH LIỄU TRUNG\", \"name\": \"LÊ NGỌC LAN\", \"gender\": \"\", \"birth\": \"1963\", \"age\": \"\", \"address\": \"\", \"tel_customer\": \"\", \"id_bhyt\": \"\", \"diagnosis\": \"\", \"drugs\": [{\"drug_name\": \"Spulit 100mg\", \"drug_dose\": \"\", \"drug_quantity\": \"60 Viên\"}, {\"drug_name\": \"Ziaja Med Anti-imperfections Formula Cleansing Body Gel (Gel tắm ngừa khuẩn) 400ml\", \"drug_dose\": \"\", \"drug_quantity\": \"1 Chai\"}, {\"drug_name\": \"Notis Antidanruff Shampoo 125ml\", \"drug_dose\": \"\", \"drug_quantity\": \"2 Chai\"}, {\"drug_name\": \"Amisea 167mg\", \"drug_dose\": \"\", \"drug_quantity\": \"30 Viên\"}, {\"drug_name\": \"Cafunten 10g\", \"drug_dose\": \"\", \"drug_quantity\": \"4 Tuýp\"}], \"date_in\": \"Ngày 26 tháng 04 năm 2022\", \"doctor_name\": \"Ths.BS TRỊNH MINH TRANG\"}\n",
      "------------------------ 2 --------------------\n",
      " /home/vinbig/Documents/PA_Modeling/Prompt/private_test_Pharma/ocr_text/20210503_205658408597.txt\n",
      "Generating...\n",
      "{\"current_institute\": \"\", \"name\": \"TRẦN XUÂN SONG\", \"gender\": \"Nam\", \"birth\": \"1945\", \"age\": \"\", \"address\": \"Thị trấn Vĩnh Tru-Lý Nhân - Hà Nam\", \"tel_customer\": \"\", \"id_bhyt\": \"HT 2 35 207 64485\", \"diagnosis\": \"Tăng huyết áp vô căn (nguyên phát):(G45) Cơn thiếu máu não thoáng qua và hội chứng liên quan; (E78) Rối loạn chuyển hoá lipoprotein và tình trạng tăng lipid máu khác\", \"drugs\": [{\"drug_name\": \"EBITAC 12.5 10mg + 12,5mg SL:30 Viên\", \"drug_dose\": \"Uống: Sáng 1 Viên\", \"drug_quantity\": \"30 Viên\"}, {\"drug_name\": \"CERECAPS SL:20 Viên +280mg+375mg+15mg\", \"drug_dose\": \"Uống: Sáng Viên Chiều Viên\", \"drug_quantity\": \"30 Viên\"}, {\"drug_name\": \"VASTINXEPA 40MG 40mg SL: 15 Viên\", \"drug_dose\": \"Uống: Tối 1/2 Viên\", \"drug_quantity\": \"15 Viên\"}], \"date_in\": \"Ngày 20/07/2020\", \"doctor_name\": \"Trương Quốc Huân\"} industries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:148: UserWarning: Stopping on a multiple token sequence is not yet supported on transformers. The first token of the stop sequence will be used as the stop sequence string in the interim.\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:404: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:17909 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated\n",
      "Result:\n",
      "  {\"current_institute\": \"\", \"name\": \"TRẦN XUÂN SONG\", \"gender\": \"Nam\", \"birth\": \"1945\", \"age\": \"\", \"address\": \"Thị trấn Vĩnh Tru-Lý Nhân - Hà Nam\", \"tel_customer\": \"\", \"id_bhyt\": \"HT 2 35 207 64485\", \"diagnosis\": \"Tăng huyết áp vô căn (nguyên phát):(G45) Cơn thiếu máu não thoáng qua và hội chứng liên quan; (E78) Rối loạn chuyển hoá lipoprotein và tình trạng tăng lipid máu khác\", \"drugs\": [{\"drug_name\": \"EBITAC 12.5 10mg + 12,5mg SL:30 Viên\", \"drug_dose\": \"Uống: Sáng 1 Viên\", \"drug_quantity\": \"30 Viên\"}, {\"drug_name\": \"CERECAPS SL:20 Viên +280mg+375mg+15mg\", \"drug_dose\": \"Uống: Sáng Viên Chiều Viên\", \"drug_quantity\": \"30 Viên\"}, {\"drug_name\": \"VASTINXEPA 40MG 40mg SL: 15 Viên\", \"drug_dose\": \"Uống: Tối 1/2 Viên\", \"drug_quantity\": \"15 Viên\"}], \"date_in\": \"Ngày 20/07/2020\", \"doctor_name\": \"Trương Quốc Huân\"}\n",
      "------------------------ 3 --------------------\n",
      " /home/vinbig/Documents/PA_Modeling/Prompt/private_test_Pharma/ocr_text/Long_Chau_412.txt\n",
      "Generating...\n",
      "{\"current_institute\": \"CÔNG TY CP MD268 Bệnh viện Đa khoa Cuộc Sống\", \"name\": \"ĐỊA CHÍR\", \"gender\": \"Nam\", \"birth\": \"25 tháng\", \"age\": \"\", \"address\": \"Tổ 3, Phường Chiềng Cơi, Thành Phố Sơn La, Sơn La\", \"tel_customer\": \"\", \"id_bhyt\": \"TE1141421599168\", \"diagnosis\": \"nôn mất nước nặng\", \"drugs\": [{\"drug_name\": \"Bisolvon 60ml\", \"drug_dose\": \"ngày uống 2 lần mỗi lần 5ml\", \"drug_quantity\": \"1 Lọ\"}, {\"drug_name\": \"Sắt lzyziron 5,358g\", \"drug_dose\": \"ngày uống 14 giọt\", \"drug_quantity\": \"1 Gói\"}, {\"drug_name\": \"Dr4kid bổ sung dinh dưỡng TE\", \"drug_dose\": \"ngày uống 1 gói\", \"drug_quantity\": \"\"}], \"date_in\": \"Ngày 8 tháng 12 năm 2022\", \"doctor_name\": \"THỊ NÊM\"} industries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:148: UserWarning: Stopping on a multiple token sequence is not yet supported on transformers. The first token of the stop sequence will be used as the stop sequence string in the interim.\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:404: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:17909 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated\n",
      "Result:\n",
      "  {\"current_institute\": \"CÔNG TY CP MD268 Bệnh viện Đa khoa Cuộc Sống\", \"name\": \"ĐỊA CHÍR\", \"gender\": \"Nam\", \"birth\": \"25 tháng\", \"age\": \"\", \"address\": \"Tổ 3, Phường Chiềng Cơi, Thành Phố Sơn La, Sơn La\", \"tel_customer\": \"\", \"id_bhyt\": \"TE1141421599168\", \"diagnosis\": \"nôn mất nước nặng\", \"drugs\": [{\"drug_name\": \"Bisolvon 60ml\", \"drug_dose\": \"ngày uống 2 lần mỗi lần 5ml\", \"drug_quantity\": \"1 Lọ\"}, {\"drug_name\": \"Sắt lzyziron 5,358g\", \"drug_dose\": \"ngày uống 14 giọt\", \"drug_quantity\": \"1 Gói\"}, {\"drug_name\": \"Dr4kid bổ sung dinh dưỡng TE\", \"drug_dose\": \"ngày uống 1 gói\", \"drug_quantity\": \"\"}], \"date_in\": \"Ngày 8 tháng 12 năm 2022\", \"doctor_name\": \"THỊ NÊM\"}\n",
      "------------------------ 4 --------------------\n",
      " /home/vinbig/Documents/PA_Modeling/Prompt/private_test_Pharma/ocr_text/Long_Chau_376.txt\n",
      "Generating...\n",
      "{\"current_institute\": \"TP.HCM\", \"name\": \"Trần Bửu Nhân\", \"gender\": \"Nữ\", \"birth\": \"17\", \"age\": \"\", \"address\": \"65/2 CAn Dương Vương Phường 8, Quận 5, TP. Hồ Chí Minh\", \"tel_customer\": \"\", \"id_bhyt\": \"\", \"diagnosis\": \"Viêm khớp gối T\", \"drugs\": [{\"drug_name\": \"Lornoxicam 8mg (Arbuntec 8)\", \"drug_dose\": \"Ngày uống 2 lần, mỗi lần 1\", \"drug_quantity\": \"28 Viên\"}, {\"drug_name\": \"Mỗi 10ml chứa: Calci lactat pentahydrat 500mg \", \"drug_dose\": \"Ngày uống 2 lần, mỗi lần 1\", \"drug_quantity\": \"28 Ông Grow-F}\"), {\"drug_name\": \"Paracetamo 500mg + Acid Ascorbic 200mg ( 40 Viên Effer-Paralmax C)\", \"drug_dose\": \"Ngày uống 3lần, mỗi lần 1 Viên\", \"drug_quantity\": \"40 Viên\"}, {\"drug_name\": \"Alphachymotrypsin 4.2mg (Babytrim-new alpha)\", \"drug_dose\": \"Ngày uống 3 lần, mỗi lần 1 Gói1,5g\", \"drug_quantity\": \"40 Gói1,5g\"}], \"date_in\": \"08/12/2022\", \"doctor_name\": \"Lời dặn: Bs CKI Hồ Ngọc Cần Giấy nghỉ việc hưởng BHXH chỉ cấp trong ngày thực hiện khám chữa bệnh *Tái khám: 22/12/2022 Đơn thuốc giá trị mua. lĩnh thuốc trong thời hạn tối đa 05 ngày. kể từ ngày kê đơn thuốc Khám lại xin mang theo don này.\"} industries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:148: UserWarning: Stopping on a multiple token sequence is not yet supported on transformers. The first token of the stop sequence will be used as the stop sequence string in the interim.\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:404: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:17909 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated\n",
      "Result:\n",
      "  {\"current_institute\": \"TP.HCM\", \"name\": \"Trần Bửu Nhân\", \"gender\": \"Nữ\", \"birth\": \"17\", \"age\": \"\", \"address\": \"65/2 CAn Dương Vương Phường 8, Quận 5, TP. Hồ Chí Minh\", \"tel_customer\": \"\", \"id_bhyt\": \"\", \"diagnosis\": \"Viêm khớp gối T\", \"drugs\": [{\"drug_name\": \"Lornoxicam 8mg (Arbuntec 8)\", \"drug_dose\": \"Ngày uống 2 lần, mỗi lần 1\", \"drug_quantity\": \"28 Viên\"}, {\"drug_name\": \"Mỗi 10ml chứa: Calci lactat pentahydrat 500mg \", \"drug_dose\": \"Ngày uống 2 lần, mỗi lần 1\", \"drug_quantity\": \"28 Ông Grow-F}\"), {\"drug_name\": \"Paracetamo 500mg + Acid Ascorbic 200mg ( 40 Viên Effer-Paralmax C)\", \"drug_dose\": \"Ngày uống 3lần, mỗi lần 1 Viên\", \"drug_quantity\": \"40 Viên\"}, {\"drug_name\": \"Alphachymotrypsin 4.2mg (Babytrim-new alpha)\", \"drug_dose\": \"Ngày uống 3 lần, mỗi lần 1 Gói1,5g\", \"drug_quantity\": \"40 Gói1,5g\"}], \"date_in\": \"08/12/2022\", \"doctor_name\": \"Lời dặn: Bs CKI Hồ Ngọc Cần Giấy nghỉ việc hưởng BHXH chỉ cấp trong ngày thực hiện khám chữa bệnh *Tái khám: 22/12/2022 Đơn thuốc giá trị mua. lĩnh thuốc trong thời hạn tối đa 05 ngày. kể từ ngày kê đơn thuốc Khám lại xin mang theo don này.\"}\n",
      "------------------------ 5 --------------------\n",
      " /home/vinbig/Documents/PA_Modeling/Prompt/private_test_Pharma/ocr_text/Long_Chau_63.txt\n",
      "Generating...\n",
      "{\"current_institute\": \"Yersin Số 10 Đường Trương Đinh Phường 6, Quận 3 Thành Phố Hồ Chí Min Việt Nam\", \"name\": \"PHÙNG THỊ LƯU\", \"gender\": \"NỮ\", \"birth\": \"1951\", \"address\": \"SỐ 2 HƯNG GIA 1,P. TÂN PHONG, QUẬN7\", \"diagnosis\": \"VIÊM DẠ DÀY-HP(+)\", \"drugs\": [{\"drug_name\": \"NEXIUM MUPS 40mg\", \"drug_dose\": \"uống 02 lần, mỗi lần 01 viên - trước ăn 30 phút- 1 giờ\", \"drug_quantity\": \"60 viên\"}, {\"drug_name\": \"AMEBISMO\", \"drug_dose\": \"uống 02 lần, mỗi lần 02 viên - nhai nuốt trước ăn 30 phút- 1 giờ\", \"drug_quantity\": \"60 viên\"}, {\"drug_name\": \"TETRACYCLIN 500mg\", \"drug_dose\": \"uống 02 lần, mỗi lần 02 viên sau ăn 30 phút-1 giờ\", \"drug_quantity\": \"60 viên\"}, {\"drug_name\": \"TINIDAZOLE 500mg\", \"drug_dose\": \"uống 02 lần, mỗi lần 01 viên sau ăn 30 phút-1giờ \", \"drug_quantity\": \"30 viên\"}, {\"drug_name\": \"MOTILIUM-M\", \"drug_dose\": \"uống 02 lần, mỗi lần 01 viên - trước ăn 30 phút-1 giờ\", \"drug_quantity\": \"30 viên\"}, {\"drug_name\": \"ENTEROGERMINA\", \"drug_dose\": \"uống 02 lần, mỗi lần 01 ống - trước ăn 30 phút-1 giờ\", \"drug_quantity\": \"30 ống\"}], \"date_in\": \"Ngày 28 tháng 11 năm 2023\", \"doctor_name\": \"BS. TRẦN VĂN HUY\"} industries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:148: UserWarning: Stopping on a multiple token sequence is not yet supported on transformers. The first token of the stop sequence will be used as the stop sequence string in the interim.\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:404: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:17909 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated\n",
      "Result:\n",
      "  {\"current_institute\": \"Yersin Số 10 Đường Trương Đinh Phường 6, Quận 3 Thành Phố Hồ Chí Min Việt Nam\", \"name\": \"PHÙNG THỊ LƯU\", \"gender\": \"NỮ\", \"birth\": \"1951\", \"address\": \"SỐ 2 HƯNG GIA 1,P. TÂN PHONG, QUẬN7\", \"diagnosis\": \"VIÊM DẠ DÀY-HP(+)\", \"drugs\": [{\"drug_name\": \"NEXIUM MUPS 40mg\", \"drug_dose\": \"uống 02 lần, mỗi lần 01 viên - trước ăn 30 phút- 1 giờ\", \"drug_quantity\": \"60 viên\"}, {\"drug_name\": \"AMEBISMO\", \"drug_dose\": \"uống 02 lần, mỗi lần 02 viên - nhai nuốt trước ăn 30 phút- 1 giờ\", \"drug_quantity\": \"60 viên\"}, {\"drug_name\": \"TETRACYCLIN 500mg\", \"drug_dose\": \"uống 02 lần, mỗi lần 02 viên sau ăn 30 phút-1 giờ\", \"drug_quantity\": \"60 viên\"}, {\"drug_name\": \"TINIDAZOLE 500mg\", \"drug_dose\": \"uống 02 lần, mỗi lần 01 viên sau ăn 30 phút-1giờ \", \"drug_quantity\": \"30 viên\"}, {\"drug_name\": \"MOTILIUM-M\", \"drug_dose\": \"uống 02 lần, mỗi lần 01 viên - trước ăn 30 phút-1 giờ\", \"drug_quantity\": \"30 viên\"}, {\"drug_name\": \"ENTEROGERMINA\", \"drug_dose\": \"uống 02 lần, mỗi lần 01 ống - trước ăn 30 phút-1 giờ\", \"drug_quantity\": \"30 ống\"}], \"date_in\": \"Ngày 28 tháng 11 năm 2023\", \"doctor_name\": \"BS. TRẦN VĂN HUY\"}\n",
      "------------------------ 6 --------------------\n",
      " /home/vinbig/Documents/PA_Modeling/Prompt/private_test_Pharma/ocr_text/Long_Chau_265.txt\n",
      "Generating...\n",
      "{\"current_institute\": \"SUYIE TP. HU CHI MINH\", \"name\": \"MS: 17D/BV-U1\", \"gender\": \"\", \"birth\": \"\", \"age\": \"\", \"address\": \"TP. HU CHI MINH\", \"tel_customer\": \"\", \"id_bhyt\": \"\", \"diagnosis\": \"\", \"drugs\": [{\"drug_name\": \"Macrogol 3350 5g\", \"drug_dose\": \"\", \"drug_quantity\": \"10 Gói\"}, {\"drug_name\": \"Inuline+Fructo Oligosaccharide+Gaaact Oligosaccharide 3g\", \"drug_dose\": \"\", \"drug_quantity\": \"20 Gói\"}], \"date_in\": \"\", \"doctor_name\": \"Nguyễn Ngọc Thúy Uyên\"} industries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:148: UserWarning: Stopping on a multiple token sequence is not yet supported on transformers. The first token of the stop sequence will be used as the stop sequence string in the interim.\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:404: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:17909 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated\n",
      "Result:\n",
      "  {\"current_institute\": \"SUYIE TP. HU CHI MINH\", \"name\": \"MS: 17D/BV-U1\", \"gender\": \"\", \"birth\": \"\", \"age\": \"\", \"address\": \"TP. HU CHI MINH\", \"tel_customer\": \"\", \"id_bhyt\": \"\", \"diagnosis\": \"\", \"drugs\": [{\"drug_name\": \"Macrogol 3350 5g\", \"drug_dose\": \"\", \"drug_quantity\": \"10 Gói\"}, {\"drug_name\": \"Inuline+Fructo Oligosaccharide+Gaaact Oligosaccharide 3g\", \"drug_dose\": \"\", \"drug_quantity\": \"20 Gói\"}], \"date_in\": \"\", \"doctor_name\": \"Nguyễn Ngọc Thúy Uyên\"}\n",
      "------------------------ 7 --------------------\n",
      " /home/vinbig/Documents/PA_Modeling/Prompt/private_test_Pharma/ocr_text/20210503_205658397640.txt\n",
      "Generating...\n",
      "{\"current_institute\": \"TOA THUÔC BHYT\", \"name\": \"TRẦN THỊ THOÁN\", \"gender\": \"Nam\", \"birth\": \"1961\", \"age\": \"\", \"address\": \"Xã Trần Hưng Đạo- Huyện Lý Nhân- Tỉnh Hà Nam\", \"tel_customer\": \"\", \"id_bhyt\": \"GB 4 35 35 206 98684\", \"diagnosis\": \"110 - Bệnh lý tăng huyết áp;(G46*) Hội chứng mạch máu não trong bệnh mạch não (I60- 167+)\", \"drugs\": [{\"drug_name\": \"SAVILOSARTAN 100 100mg SL\", \"drug_dose\": \"10 Viên\", \"drug_quantity\": \"\"}, {\"drug_name\": \"CERECAPS SL:20 Viên +280mg+375mg+15mg\", \"drug_dose\": \"1 Viên sáng, 1 Viên chiều\", \"drug_quantity\": \"\"}], \"date_in\": \"Ngày 13/04/2021\", \"doctor_name\": \"Trương Văn Nghĩa\"} industries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:148: UserWarning: Stopping on a multiple token sequence is not yet supported on transformers. The first token of the stop sequence will be used as the stop sequence string in the interim.\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:404: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:17909 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated\n",
      "Result:\n",
      "  {\"current_institute\": \"TOA THUÔC BHYT\", \"name\": \"TRẦN THỊ THOÁN\", \"gender\": \"Nam\", \"birth\": \"1961\", \"age\": \"\", \"address\": \"Xã Trần Hưng Đạo- Huyện Lý Nhân- Tỉnh Hà Nam\", \"tel_customer\": \"\", \"id_bhyt\": \"GB 4 35 35 206 98684\", \"diagnosis\": \"110 - Bệnh lý tăng huyết áp;(G46*) Hội chứng mạch máu não trong bệnh mạch não (I60- 167+)\", \"drugs\": [{\"drug_name\": \"SAVILOSARTAN 100 100mg SL\", \"drug_dose\": \"10 Viên\", \"drug_quantity\": \"\"}, {\"drug_name\": \"CERECAPS SL:20 Viên +280mg+375mg+15mg\", \"drug_dose\": \"1 Viên sáng, 1 Viên chiều\", \"drug_quantity\": \"\"}], \"date_in\": \"Ngày 13/04/2021\", \"doctor_name\": \"Trương Văn Nghĩa\"}\n",
      "------------------------ 8 --------------------\n",
      " /home/vinbig/Documents/PA_Modeling/Prompt/private_test_Pharma/ocr_text/Long_Chau_209.txt\n",
      "Generating...\n",
      "{\"current_institute\": \"SỞ Y TẾ TP. HỒ CHÍ MINH\", \"name\": \"Phái: Nam\", \"birth\": \"1236/8/108 LÊ VĂN LƯƠNG\", \"age\": \"27\", \"address\": \"Xã Phước Kiển, Huyện Nhà Bè, Hồ Chí Minh\", \"tel_customer\": \"0933 047 668\", \"id_bhyt\": \"\", \"diagnosis\": \"Viêm tiểu phế quản cấp Dị ứng thuốc:Không\", \"drugs\": [{\"drug_name\": \"Amoxicillin+ Acid Clavulanic (400/57)mg/5 ml)\", \"drug_dose\": \"Sáng 5 ml; Tối 5ml; (Pha nước đến vạch 70ml trên chai thuốc, lắc đều)\", \"drug_quantity\": \"1 Lọ\"}, {\"drug_name\": \"Salbutamo 2mg/5ml\", \"drug_dose\": \"Sáng 2 ml; Chiều 2 ml; Tối 2 ml;\", \"drug_quantity\": \"1 Chai\"}, {\"drug_name\": \"Prednisolon\", \"drug_dose\": \"Sáng 1 Gói; (sau ăn) \", \"drug_quantity\": \"3 Gói\"}], \"date_in\": \"Ngày 12 tháng 08 năm 2022\", \"doctor_name\": \"Nguyễn Trần Quỳnh Như\"} industries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:148: UserWarning: Stopping on a multiple token sequence is not yet supported on transformers. The first token of the stop sequence will be used as the stop sequence string in the interim.\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:404: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:17909 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated\n",
      "Result:\n",
      "  {\"current_institute\": \"SỞ Y TẾ TP. HỒ CHÍ MINH\", \"name\": \"Phái: Nam\", \"birth\": \"1236/8/108 LÊ VĂN LƯƠNG\", \"age\": \"27\", \"address\": \"Xã Phước Kiển, Huyện Nhà Bè, Hồ Chí Minh\", \"tel_customer\": \"0933 047 668\", \"id_bhyt\": \"\", \"diagnosis\": \"Viêm tiểu phế quản cấp Dị ứng thuốc:Không\", \"drugs\": [{\"drug_name\": \"Amoxicillin+ Acid Clavulanic (400/57)mg/5 ml)\", \"drug_dose\": \"Sáng 5 ml; Tối 5ml; (Pha nước đến vạch 70ml trên chai thuốc, lắc đều)\", \"drug_quantity\": \"1 Lọ\"}, {\"drug_name\": \"Salbutamo 2mg/5ml\", \"drug_dose\": \"Sáng 2 ml; Chiều 2 ml; Tối 2 ml;\", \"drug_quantity\": \"1 Chai\"}, {\"drug_name\": \"Prednisolon\", \"drug_dose\": \"Sáng 1 Gói; (sau ăn) \", \"drug_quantity\": \"3 Gói\"}], \"date_in\": \"Ngày 12 tháng 08 năm 2022\", \"doctor_name\": \"Nguyễn Trần Quỳnh Như\"}\n",
      "------------------------ 9 --------------------\n",
      " /home/vinbig/Documents/PA_Modeling/Prompt/private_test_Pharma/ocr_text/Long_Chau_164.txt\n",
      "Generating...\n",
      "{\"current_institute\": \"Bệnh Viện Nhi Đồng 1\", \"name\": \"ĐINH LÊ HOÀNG THIÊN\", \"gender\": \"Nam\", \"birth\": \"11\", \"age\": \"11\", \"address\": \"Ya Ba-, Huyện Ia Grai, Gia Lai\", \"tel_customer\": \"\", \"id_bhyt\": \"\", \"diagnosis\": \"Viêm tiểu phế quản cấp-J21\", \"drugs\": [{\"drug_name\": \"Erythromycin\", \"drug_dose\": \"Sáng 1/2 Gói; Chiều 1/2 Gói; Tối 1/2 Gói; \", \"drug_quantity\": \"8 Gói\"}, {\"drug_name\": \"Salbutamo\", \"drug_dose\": \"Sáng 2 ml; Chiều 2 ml; Tối 2 ml; \", \"drug_quantity\": \"1 Chai\"}, {\"drug_name\": \"Prednisolon natri phosphat\", \"drug_dose\": \"Sáng 1 Viên; (lúc no); \", \"drug_quantity\": \"5 Viên\"}, {\"drug_name\": \"Cao khô lá thường xuân\", \"drug_dose\": \"Sáng 1/2 Gói; Tối 1/2 Gói; \", \"drug_quantity\": \"5 Gói\"}], \"date_in\": \"Ngày 15 tháng 11 năm 2022\", \"doctor_name\": \"Võ Ngọc Mạnh\"} industries\n",
      "Generated\n",
      "Result:\n",
      "  {\"current_institute\": \"Bệnh Viện Nhi Đồng 1\", \"name\": \"ĐINH LÊ HOÀNG THIÊN\", \"gender\": \"Nam\", \"birth\": \"11\", \"age\": \"11\", \"address\": \"Ya Ba-, Huyện Ia Grai, Gia Lai\", \"tel_customer\": \"\", \"id_bhyt\": \"\", \"diagnosis\": \"Viêm tiểu phế quản cấp-J21\", \"drugs\": [{\"drug_name\": \"Erythromycin\", \"drug_dose\": \"Sáng 1/2 Gói; Chiều 1/2 Gói; Tối 1/2 Gói; \", \"drug_quantity\": \"8 Gói\"}, {\"drug_name\": \"Salbutamo\", \"drug_dose\": \"Sáng 2 ml; Chiều 2 ml; Tối 2 ml; \", \"drug_quantity\": \"1 Chai\"}, {\"drug_name\": \"Prednisolon natri phosphat\", \"drug_dose\": \"Sáng 1 Viên; (lúc no); \", \"drug_quantity\": \"5 Viên\"}, {\"drug_name\": \"Cao khô lá thường xuân\", \"drug_dose\": \"Sáng 1/2 Gói; Tối 1/2 Gói; \", \"drug_quantity\": \"5 Gói\"}], \"date_in\": \"Ngày 15 tháng 11 năm 2022\", \"doctor_name\": \"Võ Ngọc Mạnh\"}\n"
     ]
    }
   ],
   "source": [
    "multiprocessing.freeze_support()\n",
    "path = '/home/vinbig/Documents/PA_Modeling/Prompt/private_test_Pharma/ocr_text'\n",
    "names = os.listdir(path)\n",
    "names = names[5:15]\n",
    "results = []\n",
    "path_inputs = [os.path.join(path, name) for name in names]\n",
    "for i, path_input in enumerate(path_inputs):\n",
    "    print(\"------------------------\", i, '--------------------\\n', path_input)\n",
    "    result = main(path_input)\n",
    "    results.append(result)\n",
    "    print(\"Result:\\n\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Long_Chau_501.txt'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Indust', 1026)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tokenizer.encode(results)\n",
    "tokenizer.decode(a[-5]), len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, StoppingCriteriaList, StoppingCriteria\n",
    "\n",
    "stop_token_ids = [\n",
    "    tokenizer.convert_tokens_to_ids(x) for x in [\n",
    "        ['Human', ':'], ['AI ASSISTANT', ':']\n",
    "    ]\n",
    "]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
    "\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# define custom stopping criteria object\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_ids in stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bệnh Viện BỆNH VIỆN HOÀN MỸ ĐÀ NẴNG ID: 220014942 Hoàn Mỹ 291 Nguyễn Văn Linh,Q. Thanh Khê, Tp. Đà Nẵng Lần khám: 7 Đà Nẵng Email: Website: www.hoanmydaanag.com Chuyển CK: PK Nhi ĐƠN THUỐC 2211290213 Họ tên: NGUYỄN NGỌC AN NHIÊN Ngày sinh Tuổi :8 tháng Cân nặng :7KK Giới tính :Nữ Số Thẻ Bảo Hiểm Y Tế :TE145452092689845719 Địa chỉ :Fpt Plaza, Võ Quí Huân, Phường Hoà Hải, Quận Ngũ Hành Sơn, Đà Nẵng Chẩn đoán: viêm phổi/ viêm tai giữa tái khám (H66, J18) Thuốc điều trị: 1. Augmentin (Amoxicilin + acid clavulanic) (250+31,25) mg 14 Gói Uống Sáng: 01 Gói; Tối: 01 Gói; 2. Bacillus claussii (Enterogermina) 2 tỉ bào tử/5 mL Uống Sáng: 01 Ông; Tối: 01 Ông; -TRÁNH NGỦ LẠNH, GIỮ ÂM, TRÁNH QUẠT THÔI NGANG MẶT. XỊT MŨI VỆ SINH BẰNG *Lời dặn: Ngày 29 tháng 11/năm 2022 Bác sĩ NƯỚC MUỐI SINHLÝ 4-5 LẦN/ NGÀY Bác sĩ Trương Đức Hiến *Tái Khám:4/12/2022 Tái khám chuyên khoa :CK NHI *Tên bố hoặc mẹ của trẻ hoặc bênh *Tái khám thoại xin liên mang ITHE '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_conversation('/home/vinbig/Documents/PA_Modeling/Prompt/private_test_Pharma/ocr_text/Long_Chau_80.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\"current_institute\": \"TP. HỒ CHÍ MINH BỘ Y TẾ NHI ĐỒNG 2\", \"patient_name\": \"NGUYỄN HÀ CHI\", \"gender\": \"Nữ\", \"birth\": \"6 6t5th\", \"age\": \"6\", \"address\": \"88/33 Vũ Tùng, Phường 2, Q.Bình Thạnh, TP. Hồ Chí Minh\", \"tel_customer\": \"0987675870\", \"id_bhyt\": \"\", \"diagnosis\": \"J18.0 - Viêm phế quản phổi, không đặc hiệu\", \"drugs\": [{\"drug_name\": \"Clarithromycin (\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate TED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def get_label(path):\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Các trường cần trích xuất\n",
    "    fields = [\n",
    "        'current institute',\n",
    "        'name',\n",
    "        'gender',\n",
    "        'birth',\n",
    "        'address',\n",
    "        'id bhyt',\n",
    "        'diagnosis',\n",
    "        'date in',\n",
    "        'doctor name'\n",
    "    ]\n",
    "\n",
    "    # Tạo dictionary kết quả\n",
    "    result = {}\n",
    "    for field in fields:\n",
    "        if field in data:\n",
    "            result[field] = data[field]['value']\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'current institute': 'BỆNH VIỆN HOÀN MỸ ĐÀ NĂNG',\n",
       " 'name': 'NGUYỄN NGỌC AN NHIÊN',\n",
       " 'gender': 'Nữ',\n",
       " 'birth': '08/03/2022',\n",
       " 'address': 'Fpt Plaza, Võ Quí Huân, Phường Hoà Hải, Quận Ngũ Hành Sơn, Đà Nẵng',\n",
       " 'id bhyt': 'TE145452092689845719',\n",
       " 'diagnosis': 'viêm phổi/ viêm tai giữa tái khám (H66, J18)',\n",
       " 'date in': 'Ngày 29 tháng 11 năm 2022',\n",
       " 'doctor name': 'Bác sĩ Trương Đức Hiển'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_path = '/home/vinbig/Documents/PA_Modeling/Prompt/prescription_label_text/KIEs/Long_Chau_80.json'\n",
    "label = get_label(label_path)\n",
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Unterminated string starting at: line 1 column 340 (char 339)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m pred\n",
      "File \u001b[0;32m~/miniconda3/envs/prompt/lib/python3.9/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/miniconda3/envs/prompt/lib/python3.9/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m~/miniconda3/envs/prompt/lib/python3.9/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Unterminated string starting at: line 1 column 340 (char 339)"
     ]
    }
   ],
   "source": [
    "pred = json.loads(results)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'current_institute': 'Bệnh Viện Hoàn Mỹ',\n",
       " 'name': 'NGUYỄN NGỌC AN NHIÊN',\n",
       " 'gender': '',\n",
       " 'birth': '8 tháng',\n",
       " 'age': '8 tháng',\n",
       " 'address': 'Fpt Plaza, Võ Quí Huân, Phường Hòa Hải, Quận Ngũ Hành Sơn, Đà Nẵng',\n",
       " 'tel_customer': 'TE145452092689845719',\n",
       " 'id_bhyt': 'TE145452092689845719',\n",
       " 'diagnosis': 'viêm phổi/ viêm tai giữa tái khám (H66, J18)',\n",
       " 'date_in': 'Ngày 29 tháng 11 năm 2022',\n",
       " 'doctor_name': 'Trương Đức Hiến'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_json = json.loads(results)\n",
    "result_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "distance() got an unexpected keyword argument 'return_operations'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m JSONParseEvaluator()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcal_acc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manswer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 221\u001b[0m, in \u001b[0;36mJSONParseEvaluator.cal_acc\u001b[0;34m(self, pred, answer)\u001b[0m\n\u001b[1;32m    215\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstruct_tree_from_dict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_dict(pred))\n\u001b[1;32m    216\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstruct_tree_from_dict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_dict(answer))\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmax\u001b[39m(\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;241m-\u001b[39m (\n\u001b[0;32m--> 221\u001b[0m         \u001b[43mzss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m            \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m            \u001b[49m\u001b[43mget_children\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_children\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m            \u001b[49m\u001b[43minsert_cost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert_and_remove_cost\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m            \u001b[49m\u001b[43mremove_cost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert_and_remove_cost\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m            \u001b[49m\u001b[43mupdate_cost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_cost\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_operations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;241m/\u001b[39m zss\u001b[38;5;241m.\u001b[39mdistance(\n\u001b[1;32m    231\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstruct_tree_from_dict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_dict({})),\n\u001b[1;32m    232\u001b[0m             answer,\n\u001b[1;32m    233\u001b[0m             get_children\u001b[38;5;241m=\u001b[39mzss\u001b[38;5;241m.\u001b[39mNode\u001b[38;5;241m.\u001b[39mget_children,\n\u001b[1;32m    234\u001b[0m             insert_cost\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minsert_and_remove_cost,\n\u001b[1;32m    235\u001b[0m             remove_cost\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minsert_and_remove_cost,\n\u001b[1;32m    236\u001b[0m             update_cost\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_cost,\n\u001b[1;32m    237\u001b[0m             return_operations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    238\u001b[0m         )\n\u001b[1;32m    239\u001b[0m     ),\n\u001b[1;32m    240\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: distance() got an unexpected keyword argument 'return_operations'"
     ]
    }
   ],
   "source": [
    "evaluator = JSONParseEvaluator()\n",
    "\n",
    "evaluator.cal_acc(pred=pred, answer=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cae0f041aa74a538b856f7e5716c118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<__main__.MyStoppingCriteria object at 0x7fc241697fd0>]\n",
      "\n",
      "!!! current device is cuda !!!\n",
      "!!! Pipeline Setup Completed !!!\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:\n",
      "Human:\n",
      "AI:\n",
      "Human:\n",
      "AI:\n",
      "Human:\n",
      "AI:\n",
      "Human:\n",
      "AI:\n",
      "Human:\n",
      "AI:\n",
      "Human:\n",
      "AI:\n",
      "Human:\n",
      "AI:\n",
      "Human:\n",
      "AI:\n",
      "Human:\n",
      "AI:\n",
      "Human:\n",
      "AI:\n",
      "Human:\n",
      "AI:\n",
      "Human:\n",
      "AI:\n",
      "Human:\n",
      "AI:\n",
      "Human:\n",
      "AI:\n",
      "Human:\n",
      "AI:\n",
      "Human:\n",
      "AI:\n",
      "Human:\n",
      "AI:\n",
      "Human:\n",
      "AI:\n",
      "Human:\n",
      "AI:\n",
      "Human:\n",
      "AI:\n",
      "Human:\n",
      "AI:\n",
      "Human:\n",
      "AI:\n",
      "Human:\n",
      "AI:\n",
      "Human:\n",
      "AI:\n",
      "Human:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 262, but `max_length` is set to 256. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 270, but `max_length` is set to 256. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 278, but `max_length` is set to 256. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 291, but `max_length` is set to 256. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 303, but `max_length` is set to 256. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 310, but `max_length` is set to 256. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 318, but `max_length` is set to 256. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 326, but `max_length` is set to 256. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 334, but `max_length` is set to 256. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 342, but `max_length` is set to 256. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 350, but `max_length` is set to 256. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 358, but `max_length` is set to 256. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 366, but `max_length` is set to 256. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/vinbig/miniconda3/envs/prompt/lib/python3.9/site-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 374, but `max_length` is set to 256. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFacePipeline, PromptTemplate, LLMChain\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
    "from transformers import StoppingCriteria\n",
    "from transformers import StoppingCriteriaList\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "def model_setup(model_id:str):\n",
    "    nf4_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "\n",
    "    # モデル&トークナイザーのダウンロード\n",
    "    # model_id = 'mychen76/mistral7b_ocr_to_json_v1'\n",
    "    cache_dir = '/u01/tmp/mistral_7b_json'\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=nf4_config,\n",
    "        # device_map=\"auto\",\n",
    "        cache_dir=cache_dir,\n",
    "        \n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=cache_dir)\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        \n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def pipeline_setup(model, tokenizer, isGPU:bool, **kwargs) -> HuggingFacePipeline:\n",
    "    # GPUの確認\n",
    "    if isGPU:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"\\n!!! current device is {device} !!!\")\n",
    "        model = model\n",
    "        \n",
    "        # GPUにモデルを展開する際に必要な引数を追加\n",
    "        device = 0\n",
    "        framework = 'pt'\n",
    "    else:\n",
    "        device = -1\n",
    "        framework = None\n",
    "        \n",
    "        \n",
    "    # パイプラインの作成\n",
    "    task = \"text-generation\"\n",
    "    pipe = pipeline(\n",
    "        task,\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        # device=device,\n",
    "        framework=framework,\n",
    "        pad_token_id=0,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    # LLMs: LangChainで利用可能な形に変換\n",
    "    llm = HuggingFacePipeline(pipeline=pipe)\n",
    "    \n",
    "    print(\"!!! Pipeline Setup Completed !!!\\n\\n\")\n",
    "    \n",
    "    return llm\n",
    "\n",
    "\n",
    "\n",
    "# Stopの条件を設定するクラスを作成 (StoppingCriteriaを継承する)\n",
    "class MyStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, stop_str, num_iter, tokenizer, isGPU):\n",
    "        if isGPU:\n",
    "            self.stop_token_ids = tokenizer(stop_str, return_tensors='pt')[\"input_ids\"].to('cuda')\n",
    "            self.stop_token_ids_iter = tokenizer(stop_str*2, return_tensors='pt')[\"input_ids\"].to('cuda')\n",
    "        else:\n",
    "            self.stop_token_ids = tokenizer(stop_str, return_tensors='pt')[\"input_ids\"]\n",
    "            self.stop_token_ids_iter = tokenizer(stop_str, return_tensors='pt')[\"input_ids\"]\n",
    "            \n",
    "        self.num_iter = num_iter\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __call__(self, input_ids:torch.LongTensor, score:torch.FloatTensor, **kwargs):\n",
    "        # 出力の最後尾の文字列とstop_strが一致した回数\n",
    "        match_count = 0\n",
    "        \n",
    "        # 出力文字列を最後尾から順に、num_iterで指定された要素数だけ処理する\n",
    "        for i in range(1, self.num_iter+1): \n",
    "            input_id = input_ids[0][-i]\n",
    "            stop_id = self.stop_token_ids[0][0]\n",
    "            stop_iter_id = self.stop_token_ids_iter[0][0]\n",
    "            \n",
    "            # 対象文字列とstop_strが一致した場合、カウントを増やす\n",
    "            if input_id == stop_id:\n",
    "                match_count += 1\n",
    "            \n",
    "        # \\nが2回続いた場合、または\\n\\nが現れた場合、generate()をStopする\n",
    "        if match_count == self.num_iter or input_id == stop_iter_id:\n",
    "            isStop = True\n",
    "            # print(f\"!!! Generate() Stopped !!!\\n!!!!!!!!!\\n{self.tokenizer.decode(input_ids[0])} \\n!!!!!!!!!\")\n",
    "        else:\n",
    "            isStop = False\n",
    "        return isStop\n",
    "    \n",
    "    \n",
    "# def chat_chain_setup(template, llm) -> LLMChain:\n",
    "#     # Memory: メモリ上に会話を記録する設定\n",
    "#     memory_key = \"chat_history\"\n",
    "#     memory = ConversationBufferMemory(memory_key=memory_key, ai_prefix=\"\")\n",
    "    \n",
    "#     # Prompts: プロンプトを作成\n",
    "#     prompt = PromptTemplate(template=template, input_variables=[\"chat_history\", \"input\"])\n",
    "\n",
    "#     # Chains: プロンプト&モデル&メモリをチェーンに登録\n",
    "#     llm_chain = LLMChain(\n",
    "#         llm=llm,\n",
    "#         prompt=prompt,\n",
    "#         memory=memory\n",
    "#     )\n",
    "    \n",
    "#     return llm_chain\n",
    "\n",
    "\n",
    "def main(isGPU=False):\n",
    "    # モデルをダウンロード\n",
    "    model_id = 'mychen76/mistral7b_ocr_to_json_v1'\n",
    "    model, tokenizer = model_setup(model_id)\n",
    "\n",
    "    # Stopの条件式に用いる文字と、その文字が何回続いたらStopするかを指定\n",
    "    stop_str = \"\\n\"\n",
    "    num_iter = 2  # \\nが2回繰り返された場合、generate()をstopする\n",
    "\n",
    "    # StoppingCriteriaListクラスのインスタンスを生成\n",
    "    stopcriteria_list = StoppingCriteriaList([MyStoppingCriteria(stop_str, num_iter, tokenizer, isGPU=True)])\n",
    "    print(stopcriteria_list)\n",
    "\n",
    "    # HuggingFacePipelineを作成\n",
    "    model_args = {\"temperature\":0.1, \"max_length\": 256, \"stopping_criteria\": stopcriteria_list}\n",
    "    llm = pipeline_setup(model=model, tokenizer=tokenizer, isGPU=isGPU, **model_args)\n",
    "\n",
    "    # プロンプトテンプレートを作成\n",
    "    template = \"\"\"\n",
    "You are an AI who responds to user Input.\n",
    "Please provide an answer to the human's question.\n",
    "Additonaly, you are having a conversation with a human based on past interactions.\n",
    "\n",
    "### Answer Sample\n",
    "Human: Hi!\n",
    "AI: Hi, nice to meet you.\n",
    "\n",
    "### Past Interactions\n",
    "{chat_history}\n",
    "\n",
    "### \n",
    "Human:{input}\n",
    "\"\"\"\n",
    "\n",
    "    # Chat用のチェーンを作成\n",
    "    llm_chain = chat_chain_setup(template, llm)\n",
    "\n",
    "    # チャット形式\n",
    "    while True:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input == \"exit\":\n",
    "            break\n",
    "        else:\n",
    "            response = llm_chain.predict(input=user_input)\n",
    "            print(response)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    try:\n",
    "        isGPU = bool(sys.argv[1])\n",
    "    except Exception as e:\n",
    "        print(f\"{str(e)}: You are using CPU\")\n",
    "        isGPU = False\n",
    "\n",
    "    main(isGPU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
